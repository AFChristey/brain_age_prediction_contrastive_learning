program: main_infonce.py
data_dir: /scratch/data-registry/medical/openbhb
save_dir:  /scratch/output/brain-age-mri
model: resnet18
# change back to 300
epochs: 50
batch_size: 32
lr: 1e-4
lr_decay: step
lr_decay_rate: 0.9
lr_decay_step: 10
optimizer: adam
momentum: 0.9
weight_decay: 1e-2
train_all: 1
method: expw
kernel: rbf
sigma: 2
trial: 0
tf: noise
noise_std: 0.1
path: local
loss_choice: dynamic
NN_nb_step_size: 0
end_NN_nb: 14
NN_nb_selection: manhattan
temp_RNC: 2
label_diff: l1
feature_sim: l2
lambda_val: 0
lambda_adv: 5e-6
grl_layer: 0



# Add noise to data - noise regularisation
# noise_std
# 0, 0.01, 0.05, 0.1, 0.5
# weight decay and learning rate

# 50 epoch for all methods, same batch size (32)
# optimise train_mae


# "beta1": {"values": [0.8, 0.9, 0.95]},
# "beta2": {"values": [0.99, 0.999, 0.9999]}

# beta_1 and beta_2
# see if these make any difference 
# if does then include, if not then dont include
# asK chatgpt what are sensible range
# try with one 

# start with 10 runs

# find best model

# then plot evaluation metric - and pick one loss function that gives the best for that evaluation metric

# Use train mae for optimiser for classiciation loss function - and pick best one for site ba score (and lowest MAE score too)



# 11:15 online meeting Tuesday. 